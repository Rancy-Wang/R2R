"""
Job script generator for LLM continuation tasks.

This script generates individual job scripts for manual launch instead of using Ray.
Each job script contains the commands to run LLM continuation on a specific data range.

Usage:
    python generate_job_scripts.py --input_path <input_csv> --output_path <output_dir> --num_jobs <num_jobs>
"""

import os
import sys
import argparse
import pandas as pd
from pathlib import Path
import logging
import json
from typing import List, Dict, Any, Tuple

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def split_data_ranges(total_samples: int, num_jobs: int) -> List[tuple]:
    """
    Split the data range into num_jobs chunks.
    
    Args:
        total_samples: Total number of data samples
        num_jobs: Number of jobs to split into
        
    Returns:
        List of (low, high) tuples representing data ranges
    """
    if num_jobs <= 0:
        raise ValueError("num_jobs must be positive")
    
    if total_samples <= 0:
        raise ValueError("total_samples must be positive")
    
    chunk_size = total_samples // num_jobs
    remainder = total_samples % num_jobs
    
    ranges = []
    current_low = 0
    
    for i in range(num_jobs):
        # Add 1 to chunk_size for the first 'remainder' jobs to distribute remainder evenly
        current_chunk_size = chunk_size + (1 if i < remainder else 0)
        current_high = current_low + current_chunk_size
        
        if current_low < total_samples:  # Only add if there's data to process
            ranges.append((current_low, min(current_high, total_samples)))
            current_low = current_high
        
    return ranges


def generate_job_script(job_id: int, 
                       low: int, 
                       high: int, 
                       gpu_ids: List[int],
                       args: argparse.Namespace,
                       output_dir: Path) -> str:
    """
    Generate a bash script for a specific job.
    
    Args:
        job_id: Job identifier
        low: Lower bound of data sample ID range (inclusive)
        high: Upper bound of data sample ID range (exclusive)
        gpu_ids: List of GPU IDs to use for this job
        args: Command line arguments
        output_dir: Directory to save the script
        
    Returns:
        Path to the generated script file
    """
    script_name = f"job_{job_id:03d}_range_{low}_to_{high}.sh"
    script_path = output_dir / script_name
    
    # Get the directory of the current script
    script_dir = Path(__file__).parent
    continuation_script = script_dir / "step_2_llm_continuation.py"
    
    # Build the command arguments
    cmd_args = [
        f"--input_path {args.input_path}",
        f"--output_path {args.output_path}",
        f"--low {low}",
        f"--high {high}",
        f"--tp_size {args.tp_size}",
        f"--dp_size {args.dp_size}",
    ]
    
    # Add model parameters
    model_params = [
        'max_tokens', 'max_new_tokens', 'gen_mem_fraction', 'verify_mem_fraction',
        'num_continuation', 'previous_context', 'common_previous_context',
        'num_samples', 'temperature', 'top_p', 'top_k', 'batch_size'
    ]
    
    for param in model_params:
        if hasattr(args, param):
            value = getattr(args, param)
            cmd_args.append(f"--{param} {value}")
    
    # Add boolean flags
    boolean_flags = ['skip_stress_divergent_token', 'is_print']
    for flag in boolean_flags:
        if hasattr(args, flag) and getattr(args, flag):
            cmd_args.append(f"--{flag}")
    
    # Create bash script content
    gpu_env = ",".join(map(str, gpu_ids))
    cmd_args_joined = ' \\\n    '.join(cmd_args)
    
    script_content = f"""#!/bin/bash

# Job {job_id}: Processing data range [{low}, {high})
# GPUs: {gpu_ids}
# Generated by generate_job_scripts.py

set -e  # Exit on error

echo "Starting Job {job_id} at $(date)"
echo "Processing data range [{low}, {high})"
echo "Using GPUs: {gpu_ids}"

# Set GPU environment
export CUDA_VISIBLE_DEVICES={gpu_env}

# Create output directory if it doesn't exist
mkdir -p {args.output_path}

# Run the LLM continuation script
python {continuation_script} \\
    {cmd_args_joined}

echo "Job {job_id} completed at $(date)"
"""
    
    # Write the script file
    with open(script_path, 'w') as f:
        f.write(script_content)
    
    # Make the script executable
    os.chmod(script_path, 0o755)
    
    logger.info(f"Generated job script: {script_path}")
    return str(script_path)


def generate_master_script(job_scripts: List[str], output_dir: Path, args: argparse.Namespace) -> str:
    """
    Generate a master script to run all jobs.
    
    Args:
        job_scripts: List of paths to individual job scripts
        output_dir: Directory to save the master script
        args: Command line arguments
        
    Returns:
        Path to the generated master script
    """
    master_script_path = output_dir / "run_all_jobs.sh"
    
    script_content = f"""#!/bin/bash

# Master script to run all LLM continuation jobs
# Generated by generate_job_scripts.py
# Total jobs: {len(job_scripts)}

set -e

echo "Starting all jobs at $(date)"
echo "Total jobs to run: {len(job_scripts)}"

# Function to run jobs in parallel
run_jobs_parallel() {{
    local max_parallel={args.max_parallel_jobs}
    local pids=()
    local job_count=0
    
    for script in "$@"; do
        # Wait if we've reached the maximum number of parallel jobs
        if [ ${{#pids[@]}} -ge $max_parallel ]; then
            wait ${{pids[0]}}
            pids=("${{pids[@]:1}}")  # Remove the first PID
        fi
        
        # Start the job in background
        echo "Starting job: $script"
        ./"$script" &
        pids+=($!)
        ((job_count++))
        
        echo "Started job $job_count/${{#}} (PID: $!)"
    done
    
    # Wait for all remaining jobs to complete
    for pid in "${{pids[@]}}"; do
        wait $pid
    done
}}

# Function to run jobs sequentially
run_jobs_sequential() {{
    local job_count=0
    for script in "$@"; do
        ((job_count++))
        echo "Running job $job_count/${{#}}: $script"
        ./"$script"
        echo "Completed job $job_count/${{#}}"
    done
}}

# Change to the script directory
cd "$(dirname "$0")"

# Job scripts array
job_scripts=(
"""
    
    # Add job script names
    for script_path in job_scripts:
        script_name = Path(script_path).name
        script_content += f'    "{script_name}"\n'
    
    script_content += f""")

# Run jobs based on the parallel flag
if [ "$1" == "--parallel" ]; then
    echo "Running jobs in parallel (max {args.max_parallel_jobs} concurrent jobs)"
    run_jobs_parallel "${{job_scripts[@]}}"
else
    echo "Running jobs sequentially"
    echo "Use '$0 --parallel' to run jobs in parallel"
    run_jobs_sequential "${{job_scripts[@]}}"
fi

echo "All jobs completed at $(date)"

# Optional: Merge results
echo "You can now merge results manually or run a merge script if available"
"""
    
    # Write the master script
    with open(master_script_path, 'w') as f:
        f.write(script_content)
    
    # Make the script executable
    os.chmod(master_script_path, 0o755)
    
    logger.info(f"Generated master script: {master_script_path}")
    return str(master_script_path)


def generate_merge_script(ranges: List[tuple], output_dir: Path, args: argparse.Namespace) -> str:
    """
    Generate a script to merge results from all jobs.
    
    Args:
        ranges: List of (low, high) tuples that were processed
        output_dir: Directory to save the script
        args: Command line arguments
        
    Returns:
        Path to the generated merge script
    """
    merge_script_path = output_dir / "merge_results.py"
    
    script_content = f'''#!/usr/bin/env python3
"""
Script to merge results from all LLM continuation jobs.
Generated by generate_job_scripts.py
"""

import pandas as pd
from pathlib import Path
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def merge_results():
    """Merge results from all workers into a single file."""
    output_path = Path("{args.output_path}")
    ranges = {ranges}
    final_filename = "generation_results_data_all_real_merged.csv"
    
    logger.info("Starting to merge results from all workers...")
    
    all_dataframes = []
    
    for low, high in ranges:
        # Look for the individual result file
        individual_file = output_path / f"generation_results_data_{{low}}_to_{{high}}_real.csv"
        
        if individual_file.exists():
            logger.info(f"Loading results from {{individual_file}}")
            df = pd.read_csv(individual_file)
            all_dataframes.append(df)
        else:
            logger.warning(f"Result file not found: {{individual_file}}")
    
    if all_dataframes:
        # Concatenate all dataframes
        merged_df = pd.concat(all_dataframes, ignore_index=True)
        
        # Save merged results
        final_path = output_path / final_filename
        merged_df.to_csv(final_path, index=False)
        
        logger.info(f"Merged {{len(all_dataframes)}} result files into {{final_path}}")
        logger.info(f"Total rows in merged file: {{len(merged_df)}}")
        
        return final_path
    else:
        logger.error("No result files found to merge!")
        return None

if __name__ == "__main__":
    merge_results()
'''
    
    # Write the merge script
    with open(merge_script_path, 'w') as f:
        f.write(script_content)
    
    # Make the script executable
    os.chmod(merge_script_path, 0o755)
    
    logger.info(f"Generated merge script: {merge_script_path}")
    return str(merge_script_path)


def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(description='Generate job scripts for LLM continuation')
    
    # Required arguments
    parser.add_argument('--input_path', type=str, required=True,
                        help='Path to input CSV file from step 1')
    parser.add_argument('--output_path', type=str, default='output/playground/continuation_distributed',
                        help='Path to store output files')
    parser.add_argument('--num_jobs', type=int, required=True,
                        help='Number of parallel jobs to create')
    parser.add_argument('--scripts_output_path', type=str, default=None,
                        help='Path to store generated scripts (default: <output_path>/job_scripts)')
    
    # GPU configuration
    parser.add_argument('--gpus_per_job', type=int, default=2,
                        help='Number of GPUs per job')
    parser.add_argument('--tp_size', type=int, default=2,
                        help='Tensor parallelism size')
    parser.add_argument('--dp_size', type=int, default=1,
                        help='Data parallelism size')
    parser.add_argument('--gpu_mapping', type=str, default=None,
                        help='JSON string mapping job_id to GPU IDs, e.g., {"0": [0,1], "1": [2,3]}')
    parser.add_argument('--auto_gpu_mapping', action='store_true',
                        help='Automatically assign GPUs assuming 8 GPUs per node')
    parser.add_argument('--max_parallel_jobs', type=int, default=4,
                        help='Maximum number of jobs to run in parallel in master script')

    # Model parameters (passed to each worker)
    parser.add_argument('--max_tokens', type=int, default=32768,
                        help='Maximum number of tokens per data sample')
    parser.add_argument('--max_new_tokens', type=int, default=8192,
                        help='Maximum number of new tokens to generate')
    parser.add_argument('--gen_mem_fraction', type=float, default=0.9,
                        help='Fraction of GPU memory to allocate for generation')
    parser.add_argument('--verify_mem_fraction', type=float, default=0.3,
                        help='Fraction of GPU memory to allocate for judging')
    parser.add_argument('--num_continuation', type=int, default=1,
                        help='Number of continuations to generate')
    parser.add_argument('--previous_context', type=int, default=0,
                        help='Number of previous sentences to include in the context')
    parser.add_argument('--common_previous_context', type=int, default=-1,
                        help='Number of previous sentences to include in the common context')
    parser.add_argument('--num_samples', type=int, default=1,
                        help='Number of small model outputs to generate for each mismatch point')
    parser.add_argument('--temperature', type=float, default=0.0,
                        help='Temperature for sampling')
    parser.add_argument('--top_p', type=float, default=1.0,
                        help='Top-p probability threshold for nucleus sampling')
    parser.add_argument('--top_k', type=int, default=-1,
                        help='Top-k sampling parameter for generation')
    parser.add_argument('--batch_size', type=int, default=32,
                        help='Number of mismatches to batch together for generation')
    parser.add_argument('--skip_stress_divergent_token', action='store_true', default=False,
                        help='Whether to skip stressing the divergent token')
    parser.add_argument('--is_print', action='store_true', default=False,
                        help='Whether to print the results')
    
    return parser.parse_args()


def main():
    """Main function to generate job scripts."""
    args = parse_args()
    
    # Create output directories
    output_path = Path(args.output_path)
    output_path.mkdir(parents=True, exist_ok=True)
    
    scripts_output_path = Path(args.scripts_output_path) if args.scripts_output_path else output_path / "job_scripts"
    scripts_output_path.mkdir(parents=True, exist_ok=True)
    
    # Save configuration
    config = args.__dict__.copy()
    with open(scripts_output_path / 'job_generation_config.json', 'w') as f:
        json.dump(config, f, indent=2)
    
    # Load input data to determine total number of samples
    logger.info(f"Loading input data from {args.input_path}")
    input_data = pd.read_csv(args.input_path)
    
    # Get unique data_ids to determine the range
    unique_data_ids = sorted(input_data['data_id'].unique())
    total_samples = len(unique_data_ids)
    min_data_id = min(unique_data_ids)
    max_data_id = max(unique_data_ids)
    
    logger.info(f"Found {total_samples} unique data samples (IDs: {min_data_id} to {max_data_id})")
    
    # Split data into ranges
    ranges = split_data_ranges(total_samples, args.num_jobs)
    logger.info(f"Split data into {len(ranges)} jobs:")
    
    # Convert index-based ranges to actual data_id ranges
    actual_ranges = []
    for i, (low, high) in enumerate(ranges):
        actual_low = unique_data_ids[low] if low < len(unique_data_ids) else unique_data_ids[-1]
        actual_high = unique_data_ids[high-1] + 1 if high <= len(unique_data_ids) else unique_data_ids[-1] + 1
        actual_ranges.append((actual_low, actual_high))
        logger.info(f"  Job {i+1}: data_id range [{actual_low}, {actual_high}) ({high-low} samples)")
    
    # Generate GPU mapping
    gpu_mapping = {}
    if args.gpu_mapping:
        gpu_mapping = json.loads(args.gpu_mapping)
    elif args.auto_gpu_mapping:
        # Auto-assign GPUs assuming 8 GPUs per node
        for i in range(len(actual_ranges)):
            start_gpu = (i * args.gpus_per_job) % 8
            gpu_list = [start_gpu + j for j in range(args.gpus_per_job)]
            gpu_mapping[str(i)] = gpu_list
    else:
        # Default: assign sequential GPUs
        for i in range(len(actual_ranges)):
            start_gpu = i * args.gpus_per_job
            gpu_list = [start_gpu + j for j in range(args.gpus_per_job)]
            gpu_mapping[str(i)] = gpu_list
    
    logger.info("GPU mapping:")
    for job_id, gpus in gpu_mapping.items():
        logger.info(f"  Job {job_id}: GPUs {gpus}")
    
    # Generate job scripts
    job_scripts = []
    for i, (low, high) in enumerate(actual_ranges):
        gpu_ids = gpu_mapping.get(str(i), [0, 1])  # Default to GPUs 0,1 if not specified
        script_path = generate_job_script(i, low, high, gpu_ids, args, scripts_output_path)
        job_scripts.append(script_path)
    
    # Generate master script
    master_script = generate_master_script(job_scripts, scripts_output_path, args)
    
    # Generate merge script
    merge_script = generate_merge_script(actual_ranges, scripts_output_path, args)
    
    # Generate summary
    summary = {
        'total_jobs': len(job_scripts),
        'total_samples': total_samples,
        'data_id_range': [min_data_id, max_data_id],
        'job_ranges': actual_ranges,
        'gpu_mapping': gpu_mapping,
        'scripts_generated': {
            'job_scripts': [Path(script).name for script in job_scripts],
            'master_script': Path(master_script).name,
            'merge_script': Path(merge_script).name
        }
    }
    
    with open(scripts_output_path / 'generation_summary.json', 'w') as f:
        json.dump(summary, f, indent=2)
    
    logger.info(f"\n=== Job Scripts Generation Complete ===")
    logger.info(f"Generated {len(job_scripts)} job scripts in: {scripts_output_path}")
    logger.info(f"Master script: {master_script}")
    logger.info(f"Merge script: {merge_script}")
    logger.info(f"\nTo run all jobs:")
    logger.info(f"  cd {scripts_output_path}")
    logger.info(f"  ./run_all_jobs.sh                    # Run sequentially")
    logger.info(f"  ./run_all_jobs.sh --parallel         # Run in parallel")
    logger.info(f"\nTo run individual jobs:")
    logger.info(f"  cd {scripts_output_path}")
    logger.info(f"  ./job_000_range_X_to_Y.sh            # Run specific job")
    logger.info(f"\nTo merge results after completion:")
    logger.info(f"  cd {scripts_output_path}")
    logger.info(f"  python merge_results.py")


if __name__ == "__main__":
    main() 